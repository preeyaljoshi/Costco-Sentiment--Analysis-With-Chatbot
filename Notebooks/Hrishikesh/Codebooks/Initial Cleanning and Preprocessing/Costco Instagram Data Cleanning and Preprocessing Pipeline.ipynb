{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Pipeline for Instagram Data\n",
    "def read_and_clean_data(file_path, output_file):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses Instagram comment data from CSV and saves a single output file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to input CSV file.\n",
    "    - output_file (str): File path for saving the output CSV.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Cleaned comments DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Drop unnecessary columns and rename\n",
    "    columns_to_drop = ['profilePictureUrl', 'username', 'profileUrl', \n",
    "                       'commentId', 'ownerId', 'timestamp']\n",
    "    df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    \n",
    "    df.columns = ['comments', 'comment_like', 'reply_count', \n",
    "                  'comment_date', 'post_urls']\n",
    "    \n",
    "    # Date processing\n",
    "    df['comment_date'] = pd.to_datetime(df['comment_date'], errors='coerce')\n",
    "    df[['comment_year', 'comment_month']] = (\n",
    "        df['comment_date'].apply(lambda x: pd.Series([x.year, x.month]))\n",
    "    )\n",
    "    df.drop(columns='comment_date', inplace=True)\n",
    "    pd.options.display.float_format = '{:.0f}'.format\n",
    "    \n",
    "    # Filter valid URLs and extract short_code\n",
    "    df = df[df['post_urls'].apply(lambda x: isinstance(x, str))]\n",
    "    df['short_code'] = df['post_urls'].str.extract(r'/p/([^/?]+)/')\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[['post_urls', 'short_code', 'comments', \n",
    "             'comment_like', 'reply_count', \n",
    "             'comment_year', 'comment_month']]\n",
    "    \n",
    "    df.index = np.arange(1, len(df) + 1)\n",
    "    \n",
    "    # Save output as a single file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved file: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_post_details(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Processes Instagram post details from CSV and saves the result.\n",
    "\n",
    "    Args:\n",
    "    - input_file (str): Path to input CSV.\n",
    "    - output_file (str): Path to save output CSV.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed post details DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        post_details = pd.read_csv(input_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Select and rename required columns\n",
    "    required_columns = ['caption', 'display_url', \n",
    "                        'like_count', 'post_date', \n",
    "                        'short_code']\n",
    "    post_details = post_details[required_columns]\n",
    "    post_details.columns = ['caption', 'display_url', \n",
    "                            'likes', 'post_date', \n",
    "                            'short_code']\n",
    "\n",
    "\n",
    "    # Add proper indexing\n",
    "    post_details.index = np.arange(1, len(post_details) + 1)\n",
    "\n",
    "    # Convert 'post_date' to datetime and extract components\n",
    "    post_details['post_date'] = pd.to_datetime(post_details['post_date'], errors='coerce')\n",
    "    post_details['post_year'] = post_details['post_date'].dt.year\n",
    "    post_details['post_month'] = post_details['post_date'].dt.month\n",
    "    \n",
    "    # Drop the original date column\n",
    "    post_details.drop(columns=['post_date'], inplace=True, errors='ignore')\n",
    "    pd.options.display.float_format = '{:.0f}'.format\n",
    "    \n",
    "    # Save output\n",
    "    post_details.to_csv(output_file, index=False)\n",
    "    print(f\"Post details saved to {output_file}\")\n",
    "    \n",
    "    return post_details\n",
    "\n",
    "\n",
    "def merging_files(comments_df, posts_df, output_file):\n",
    "    \"\"\"\n",
    "    Merges comment and post details DataFrames on 'short_code' and saves the output.\n",
    "\n",
    "    Args:\n",
    "    - comments_df (pd.DataFrame): Comments DataFrame.\n",
    "    - posts_df (pd.DataFrame): Posts DataFrame.\n",
    "    - output_file (str): Path to save merged CSV.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "    if isinstance(comments_df, str):\n",
    "        comments_df = pd.read_csv(comments_df)\n",
    "    if isinstance(posts_df, str):\n",
    "        posts_df = pd.read_csv(posts_df)\n",
    "\n",
    "    # Perform an inner merge on short_code\n",
    "    merged_df = pd.merge(comments_df, posts_df, on='short_code', how='inner')\n",
    "    \n",
    "    # Reorder columns for consistency\n",
    "    merged_df = merged_df[['short_code', 'caption',  \n",
    "                           'likes', 'post_year', 'post_month',\n",
    "                           'comments', \n",
    "                           'comment_like', 'reply_count', \n",
    "                           'comment_year', 'comment_month']]\n",
    "    \n",
    "    pd.options.display.float_format = '{:.0f}'.format\n",
    "    merged_df.index = np.arange(1, len(merged_df) + 1)\n",
    "    \n",
    "    # Save output\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Merged data saved to {output_file}\")\n",
    "    merged_df.index = np.arange(1, len(merged_df) + 1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def append_files(input_files, output_file):\n",
    "    \"\"\"\n",
    "    Appends multiple CSV files into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "    - input_files (list of str): List of file paths to append.\n",
    "    - output_file (str): Path to save the appended CSV.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"Appended: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_df.index = np.arange(1, len(combined_df) + 1)\n",
    "    \n",
    "    # Save combined CSV\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined CSV saved to: {output_file}\")\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3714195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For Comments data \n",
    "input_file = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\phathom_Insta_Costco_results.csv\"\n",
    "output_file = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\InstaCostco_Part1.csv\"\n",
    "\n",
    "Data = read_and_clean_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Posts details\n",
    "post_file = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\instagram-post-details.csv\"\n",
    "post_output = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\post_details.csv\"\n",
    "\n",
    "post_details = process_post_details(post_file, post_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf4de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Merged Output (Comments data and Post details)\n",
    "comments_df = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\InstaCostco_Part1.csv\"\n",
    "posts_df = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\post_details.csv\"\n",
    "final_output = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\Final_Part_1.csv\"\n",
    "\n",
    "final_part1 = merging_files(comments_df, posts_df, final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe2003",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa90cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display rows where 'short_code' is a float (likely NaN)\n",
    "missing_urls = final_part1[final_part1['short_code'].apply(lambda x: isinstance(x, float))]\n",
    "print(missing_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04feb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Posts details\n",
    "post_file_1 = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\remaining_instagram-post-details_1.csv\"\n",
    "post_output_1 = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\remaining_post_details.csv\"\n",
    "\n",
    "post_details1 = process_post_details(post_file_1, post_output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06625b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_details1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb70a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Merged Output (Comments data and Post details)\n",
    "comments_df1 = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\Apify_Costco_Instagram_results.csv\"\n",
    "posts_df1 = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\remaining_post_details.csv\"\n",
    "final_output1 = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\Final_Part_2.csv\"\n",
    "\n",
    "final_part2 = merging_files(comments_df1, posts_df1, final_output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa646d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display rows where 'short_code' is a float (likely NaN)\n",
    "missing_urls = final_part2[final_part2['short_code'].apply(lambda x: isinstance(x, float))]\n",
    "print(missing_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d52eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets append the final preprocessed parts (Final_Part1 and Final_Part2)\n",
    "final_part_1 = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\Final_Part_1.csv\"\n",
    "final_part_2 = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\Final_Part_2.csv\"\n",
    "\n",
    "# Output combined file\n",
    "final_combined_output = r\"C:\\Users\\hirshikesh\\Desktop\\Capstone Project\\cleaned_costco_data_initial.csv\"\n",
    "\n",
    "# Call the append function\n",
    "initial_cleaned_data = append_files(\n",
    "    input_files=[final_part_1, final_part_2], \n",
    "    output_file=final_combined_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0db2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and display rows where 'short_code' is a float (likely NaN)\n",
    "missing_urls = initial_cleaned_data[initial_cleaned_data['short_code'].apply(lambda x: isinstance(x, float))]\n",
    "print(missing_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
