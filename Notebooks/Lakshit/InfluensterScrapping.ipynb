{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94lQLnGp6t72",
        "outputId": "690bcdbd-7a3b-478a-87ed-65f7845cc45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.28.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: apt-get\n",
            "cp: /usr/lib/chromium-browser/chromedriver: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Install Selenium and ChromeDriver (run this cell only once)\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SqpzObk60TC"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "rYiMR5Oe7Wyv",
        "outputId": "d5cd38e6-e236-438c-ce6b-9898e4a2b241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opening URL: https://www.influenster.com/reviews/costco/reviews\n",
            "Page loaded successfully.\n",
            "Timeout: Reviews container not found. Saving debug page for inspection.\n",
            "Closed the browser.\n"
          ]
        },
        {
          "ename": "TimeoutException",
          "evalue": "Message: \nStacktrace:\n0   chromedriver                        0x00000001015d6138 cxxbridge1$str$ptr + 3653888\n1   chromedriver                        0x00000001015ce988 cxxbridge1$str$ptr + 3623248\n2   chromedriver                        0x0000000101034968 cxxbridge1$string$len + 89228\n3   chromedriver                        0x0000000101078d4c cxxbridge1$string$len + 368752\n4   chromedriver                        0x00000001010b24f0 cxxbridge1$string$len + 604180\n5   chromedriver                        0x000000010106d564 cxxbridge1$string$len + 321672\n6   chromedriver                        0x000000010106e1b4 cxxbridge1$string$len + 324824\n7   chromedriver                        0x00000001015a0fc0 cxxbridge1$str$ptr + 3436424\n8   chromedriver                        0x00000001015a42dc cxxbridge1$str$ptr + 3449508\n9   chromedriver                        0x0000000101587e60 cxxbridge1$str$ptr + 3333672\n10  chromedriver                        0x00000001015a4b9c cxxbridge1$str$ptr + 3451748\n11  chromedriver                        0x0000000101579678 cxxbridge1$str$ptr + 3274304\n12  chromedriver                        0x00000001015bf2b4 cxxbridge1$str$ptr + 3560060\n13  chromedriver                        0x00000001015bf430 cxxbridge1$str$ptr + 3560440\n14  chromedriver                        0x00000001015ce5fc cxxbridge1$str$ptr + 3622340\n15  libsystem_pthread.dylib             0x00000001948782e4 _pthread_start + 136\n16  libsystem_pthread.dylib             0x00000001948730fc thread_start + 8\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m wait \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m30\u001b[39m)  \u001b[38;5;66;03m# Adjust timeout as needed\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     reviews_container \u001b[38;5;241m=\u001b[39m wait\u001b[38;5;241m.\u001b[39muntil(\n\u001b[1;32m     21\u001b[0m         EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview_review__body-text__7LQFd\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviews container located.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:146\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n0   chromedriver                        0x00000001015d6138 cxxbridge1$str$ptr + 3653888\n1   chromedriver                        0x00000001015ce988 cxxbridge1$str$ptr + 3623248\n2   chromedriver                        0x0000000101034968 cxxbridge1$string$len + 89228\n3   chromedriver                        0x0000000101078d4c cxxbridge1$string$len + 368752\n4   chromedriver                        0x00000001010b24f0 cxxbridge1$string$len + 604180\n5   chromedriver                        0x000000010106d564 cxxbridge1$string$len + 321672\n6   chromedriver                        0x000000010106e1b4 cxxbridge1$string$len + 324824\n7   chromedriver                        0x00000001015a0fc0 cxxbridge1$str$ptr + 3436424\n8   chromedriver                        0x00000001015a42dc cxxbridge1$str$ptr + 3449508\n9   chromedriver                        0x0000000101587e60 cxxbridge1$str$ptr + 3333672\n10  chromedriver                        0x00000001015a4b9c cxxbridge1$str$ptr + 3451748\n11  chromedriver                        0x0000000101579678 cxxbridge1$str$ptr + 3274304\n12  chromedriver                        0x00000001015bf2b4 cxxbridge1$str$ptr + 3560060\n13  chromedriver                        0x00000001015bf430 cxxbridge1$str$ptr + 3560440\n14  chromedriver                        0x00000001015ce5fc cxxbridge1$str$ptr + 3622340\n15  libsystem_pthread.dylib             0x00000001948782e4 _pthread_start + 136\n16  libsystem_pthread.dylib             0x00000001948730fc thread_start + 8\n"
          ]
        }
      ],
      "source": [
        "# Configure Selenium WebDriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "options.add_argument(\"start-maximized\")\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--no-sandbox')\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "try:\n",
        "    url = \"https://www.influenster.com/reviews/costco/reviews\"\n",
        "    print(f\"Opening URL: {url}\")\n",
        "    driver.get(url)\n",
        "\n",
        "    print(\"Page loaded successfully.\")\n",
        "\n",
        "    # Wait for the main container or some element to indicate the page is fully loaded\n",
        "    wait = WebDriverWait(driver, 30)  # Adjust timeout as needed\n",
        "    try:\n",
        "        reviews_container = wait.until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, \"Review_review__body-text__7LQFd\"))\n",
        "        )\n",
        "        print(\"Reviews container located.\")\n",
        "    except TimeoutException:\n",
        "        print(\"Timeout: Reviews container not found. Saving debug page for inspection.\")\n",
        "        with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(driver.page_source)\n",
        "        raise\n",
        "\n",
        "    # Optional: Click 'Load More' button until all reviews are loaded\n",
        "    while True:\n",
        "        try:\n",
        "            load_more_button = wait.until(\n",
        "                EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(), 'Load More')]\"))\n",
        "            )\n",
        "            load_more_button.click()\n",
        "            print(\"Clicked 'Load More' button.\")\n",
        "            time.sleep(2)  # Adjust wait time for new reviews to load\n",
        "        except TimeoutException:\n",
        "            print(\"No more 'Load More' button found or all reviews loaded.\")\n",
        "            break\n",
        "\n",
        "    # Parse the page source with BeautifulSoup\n",
        "    print(\"Parsing the page with BeautifulSoup...\")\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Extract reviews\n",
        "    reviews = soup.find_all(\"div\", class_=\"Review_review__body-text__7LQFd\")\n",
        "    print(f\"Number of reviews found: {len(reviews)}\")\n",
        "\n",
        "    # Save reviews to a CSV file\n",
        "    with open(\"costco_reviews.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"Review\"])\n",
        "        for review in reviews:\n",
        "            text = review.get_text(strip=True)\n",
        "            writer.writerow([text])\n",
        "        print(f\"Saved {len(reviews)} reviews to costco_reviews.csv.\")\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "    print(\"Closed the browser.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvZsMtRX7Yi4",
        "outputId": "a479b086-c262-4027-9755-323ea7bf3804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloudscraper in /opt/anaconda3/lib/python3.12/site-packages (1.2.71)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /opt/anaconda3/lib/python3.12/site-packages (from cloudscraper) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.9.2 in /opt/anaconda3/lib/python3.12/site-packages (from cloudscraper) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.9.2->cloudscraper) (2024.7.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install cloudscraper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDICYKsXSrz5",
        "outputId": "3390288f-35cf-4f80-b089-cf8d48c02087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (4.28.0)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
            "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install selenium pandas beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McjLff-aCRQ0",
        "outputId": "82ab583e-a6b7-4fb7-e002-bd6ef220e442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 10 reviews so far...\n",
            "Extracted 10 reviews so far...\n",
            "Extracted 20 reviews so far...\n",
            "Extracted 30 reviews so far...\n",
            "Extracted 40 reviews so far...\n",
            "Extracted 50 reviews so far...\n",
            "Extracted 60 reviews so far...\n",
            "Extracted 70 reviews so far...\n",
            "Extracted 80 reviews so far...\n",
            "Extracted 90 reviews so far...\n",
            "Extracted 100 reviews so far...\n",
            "Extracted 110 reviews so far...\n",
            "Extracted 120 reviews so far...\n",
            "Extracted 130 reviews so far...\n",
            "Extracted 140 reviews so far...\n",
            "Extracted 150 reviews so far...\n",
            "Extracted 160 reviews so far...\n",
            "Extracted 170 reviews so far...\n",
            "Extracted 180 reviews so far...\n",
            "Extracted 190 reviews so far...\n",
            "Extracted 200 reviews so far...\n",
            "Extracted 210 reviews so far...\n",
            "Extracted 220 reviews so far...\n",
            "Extracted 230 reviews so far...\n",
            "Extracted 240 reviews so far...\n",
            "Extracted 250 reviews so far...\n",
            "Extracted 260 reviews so far...\n",
            "Extracted 270 reviews so far...\n",
            "Extracted 280 reviews so far...\n",
            "Extracted 290 reviews so far...\n",
            "Extracted 300 reviews so far...\n",
            "Extracted 310 reviews so far...\n",
            "Extracted 320 reviews so far...\n",
            "Extracted 330 reviews so far...\n",
            "Extracted 340 reviews so far...\n",
            "Extracted 350 reviews so far...\n",
            "Extracted 360 reviews so far...\n",
            "Extracted 370 reviews so far...\n",
            "Extracted 380 reviews so far...\n",
            "Extracted 390 reviews so far...\n",
            "Extracted 400 reviews so far...\n",
            "Extracted 410 reviews so far...\n",
            "Extracted 420 reviews so far...\n",
            "Extracted 430 reviews so far...\n",
            "Extracted 440 reviews so far...\n",
            "Extracted 450 reviews so far...\n",
            "Extracted 460 reviews so far...\n",
            "Extracted 470 reviews so far...\n",
            "Extracted 480 reviews so far...\n",
            "Extracted 490 reviews so far...\n",
            "Extracted 500 reviews so far...\n",
            "Extracted 510 reviews so far...\n",
            "Extracted 520 reviews so far...\n",
            "Extracted 530 reviews so far...\n",
            "Extracted 540 reviews so far...\n",
            "Extracted 550 reviews so far...\n",
            "Extracted 560 reviews so far...\n",
            "Extracted 570 reviews so far...\n",
            "Extracted 580 reviews so far...\n",
            "Extracted 590 reviews so far...\n",
            "Extracted 600 reviews so far...\n",
            "Extracted 610 reviews so far...\n",
            "Extracted 620 reviews so far...\n",
            "Extracted 630 reviews so far...\n",
            "Extracted 640 reviews so far...\n",
            "Extracted 650 reviews so far...\n",
            "Extracted 660 reviews so far...\n",
            "Extracted 670 reviews so far...\n",
            "Extracted 680 reviews so far...\n",
            "Extracted 690 reviews so far...\n",
            "Extracted 700 reviews so far...\n",
            "Extracted 710 reviews so far...\n",
            "Extracted 720 reviews so far...\n",
            "Extracted 730 reviews so far...\n",
            "Extracted 740 reviews so far...\n",
            "Extracted 750 reviews so far...\n",
            "Extracted 760 reviews so far...\n",
            "Extracted 770 reviews so far...\n",
            "Extracted 780 reviews so far...\n",
            "Extracted 790 reviews so far...\n",
            "Extracted 800 reviews so far...\n",
            "Extracted 810 reviews so far...\n",
            "Extracted 820 reviews so far...\n",
            "Extracted 830 reviews so far...\n",
            "Extracted 840 reviews so far...\n",
            "Extracted 850 reviews so far...\n",
            "Extracted 860 reviews so far...\n",
            "Extracted 870 reviews so far...\n",
            "Extracted 880 reviews so far...\n",
            "Extracted 890 reviews so far...\n",
            "Extracted 900 reviews so far...\n",
            "Extracted 910 reviews so far...\n",
            "Extracted 920 reviews so far...\n",
            "Extracted 930 reviews so far...\n",
            "Extracted 940 reviews so far...\n",
            "Extracted 950 reviews so far...\n",
            "Extracted 960 reviews so far...\n",
            "Extracted 970 reviews so far...\n",
            "Extracted 980 reviews so far...\n",
            "Extracted 990 reviews so far...\n",
            "Extracted 1000 reviews so far...\n",
            "Extracted 1010 reviews so far...\n",
            "Extracted 1020 reviews so far...\n",
            "Extracted 1030 reviews so far...\n",
            "Extracted 1040 reviews so far...\n",
            "Extracted 1050 reviews so far...\n",
            "Extracted 1060 reviews so far...\n",
            "Extracted 1070 reviews so far...\n",
            "Extracted 1080 reviews so far...\n",
            "Extracted 1090 reviews so far...\n",
            "Extracted 1100 reviews so far...\n",
            "Extracted 1110 reviews so far...\n",
            "Extracted 1120 reviews so far...\n",
            "Extracted 1130 reviews so far...\n",
            "Extracted 1140 reviews so far...\n",
            "Extracted 1150 reviews so far...\n",
            "Extracted 1160 reviews so far...\n",
            "Extracted 1170 reviews so far...\n",
            "Extracted 1180 reviews so far...\n",
            "Extracted 1190 reviews so far...\n",
            "Extracted 1200 reviews so far...\n",
            "Extracted 1210 reviews so far...\n",
            "Extracted 1220 reviews so far...\n",
            "Extracted 1230 reviews so far...\n",
            "Extracted 1240 reviews so far...\n",
            "Extracted 1250 reviews so far...\n",
            "Extracted 1260 reviews so far...\n",
            "Extracted 1270 reviews so far...\n",
            "Extracted 1280 reviews so far...\n",
            "Extracted 1290 reviews so far...\n",
            "Extracted 1300 reviews so far...\n",
            "Extracted 1310 reviews so far...\n",
            "Extracted 1320 reviews so far...\n",
            "Extracted 1330 reviews so far...\n",
            "Extracted 1340 reviews so far...\n",
            "Extracted 1350 reviews so far...\n",
            "Extracted 1360 reviews so far...\n",
            "Extracted 1370 reviews so far...\n",
            "Extracted 1380 reviews so far...\n",
            "Extracted 1390 reviews so far...\n",
            "Extracted 1400 reviews so far...\n",
            "Extracted 1410 reviews so far...\n",
            "Extracted 1420 reviews so far...\n",
            "Extracted 1430 reviews so far...\n",
            "Extracted 1440 reviews so far...\n",
            "Extracted 1450 reviews so far...\n",
            "Extracted 1460 reviews so far...\n",
            "Extracted 1470 reviews so far...\n",
            "Extracted 1480 reviews so far...\n",
            "Extracted 1490 reviews so far...\n",
            "Extracted 1500 reviews so far...\n",
            "Extracted 1510 reviews so far...\n",
            "Extracted 1520 reviews so far...\n",
            "Extracted 1530 reviews so far...\n",
            "Extracted 1540 reviews so far...\n",
            "Extracted 1550 reviews so far...\n",
            "Extracted 1560 reviews so far...\n",
            "Extracted 1570 reviews so far...\n",
            "Extracted 1580 reviews so far...\n",
            "Extracted 1590 reviews so far...\n",
            "Extracted 1600 reviews so far...\n",
            "Extracted 1610 reviews so far...\n",
            "Extracted 1620 reviews so far...\n",
            "Extracted 1630 reviews so far...\n",
            "Extracted 1640 reviews so far...\n",
            "Extracted 1650 reviews so far...\n",
            "Extracted 1660 reviews so far...\n",
            "Extracted 1670 reviews so far...\n",
            "Extracted 1680 reviews so far...\n",
            "Extracted 1690 reviews so far...\n",
            "Extracted 1700 reviews so far...\n",
            "Extracted 1710 reviews so far...\n",
            "Extracted 1720 reviews so far...\n",
            "Extracted 1730 reviews so far...\n",
            "Extracted 1740 reviews so far...\n",
            "Extracted 1750 reviews so far...\n",
            "Extracted 1760 reviews so far...\n",
            "Extracted 1770 reviews so far...\n",
            "Extracted 1780 reviews so far...\n",
            "Extracted 1790 reviews so far...\n",
            "Extracted 1800 reviews so far...\n",
            "Extracted 1810 reviews so far...\n",
            "Extracted 1820 reviews so far...\n",
            "Extracted 1830 reviews so far...\n",
            "Extracted 1840 reviews so far...\n",
            "Extracted 1850 reviews so far...\n",
            "Extracted 1860 reviews so far...\n",
            "Extracted 1870 reviews so far...\n",
            "Extracted 1880 reviews so far...\n",
            "Extracted 1890 reviews so far...\n",
            "Extracted 1900 reviews so far...\n",
            "Extracted 1910 reviews so far...\n",
            "Extracted 1920 reviews so far...\n",
            "Extracted 1930 reviews so far...\n",
            "Extracted 1940 reviews so far...\n",
            "Extracted 1950 reviews so far...\n",
            "Extracted 1960 reviews so far...\n",
            "Extracted 1970 reviews so far...\n",
            "Extracted 1980 reviews so far...\n",
            "Extracted 1990 reviews so far...\n",
            "Extracted 2000 reviews so far...\n",
            "Extracted 2010 reviews so far...\n",
            "Extracted 2020 reviews so far...\n",
            "Extracted 2030 reviews so far...\n",
            "Extracted 2040 reviews so far...\n",
            "Extracted 2050 reviews so far...\n",
            "Extracted 2060 reviews so far...\n",
            "Extracted 2070 reviews so far...\n",
            "Extracted 2080 reviews so far...\n",
            "Extracted 2090 reviews so far...\n",
            "Extracted 2100 reviews so far...\n",
            "Extracted 2110 reviews so far...\n",
            "Extracted 2120 reviews so far...\n",
            "Extracted 2130 reviews so far...\n",
            "Extracted 2140 reviews so far...\n",
            "Extracted 2150 reviews so far...\n",
            "Extracted 2160 reviews so far...\n",
            "Extracted 2170 reviews so far...\n",
            "Extracted 2180 reviews so far...\n",
            "Extracted 2190 reviews so far...\n",
            "Extracted 2200 reviews so far...\n",
            "Extracted 2210 reviews so far...\n",
            "Extracted 2220 reviews so far...\n",
            "Extracted 2230 reviews so far...\n",
            "Extracted 2240 reviews so far...\n",
            "Extracted 2250 reviews so far...\n",
            "Extracted 2260 reviews so far...\n",
            "Extracted 2270 reviews so far...\n",
            "Extracted 2280 reviews so far...\n",
            "Extracted 2290 reviews so far...\n",
            "Extracted 2300 reviews so far...\n",
            "Extracted 2310 reviews so far...\n",
            "Extracted 2320 reviews so far...\n",
            "Extracted 2330 reviews so far...\n",
            "Extracted 2340 reviews so far...\n",
            "Extracted 2350 reviews so far...\n",
            "Extracted 2360 reviews so far...\n",
            "Extracted 2370 reviews so far...\n",
            "Extracted 2380 reviews so far...\n",
            "Extracted 2390 reviews so far...\n",
            "Extracted 2400 reviews so far...\n",
            "Extracted 2410 reviews so far...\n",
            "Extracted 2420 reviews so far...\n",
            "Extracted 2420 reviews so far...\n",
            "Extracted 2430 reviews so far...\n",
            "Extracted 2440 reviews so far...\n",
            "Extracted 2450 reviews so far...\n",
            "Extracted 2460 reviews so far...\n",
            "Extracted 2470 reviews so far...\n",
            "Extracted 2480 reviews so far...\n",
            "Extracted 2490 reviews so far...\n",
            "Extracted 2500 reviews so far...\n",
            "Extracted 2510 reviews so far...\n",
            "Extracted 2520 reviews so far...\n",
            "Extracted 2530 reviews so far...\n",
            "Extracted 2540 reviews so far...\n",
            "Extracted 2550 reviews so far...\n",
            "Extracted 2560 reviews so far...\n",
            "Extracted 2570 reviews so far...\n",
            "Extracted 2580 reviews so far...\n",
            "Extracted 2590 reviews so far...\n",
            "Extracted 2600 reviews so far...\n",
            "Extracted 2610 reviews so far...\n",
            "Extracted 2620 reviews so far...\n",
            "Extracted 2630 reviews so far...\n",
            "Extracted 2640 reviews so far...\n",
            "Extracted 2650 reviews so far...\n",
            "Extracted 2660 reviews so far...\n",
            "Extracted 2670 reviews so far...\n",
            "Extracted 2680 reviews so far...\n",
            "Extracted 2690 reviews so far...\n",
            "Extracted 2700 reviews so far...\n",
            "Extracted 2710 reviews so far...\n",
            "Extracted 2720 reviews so far...\n",
            "Extracted 2730 reviews so far...\n",
            "Extracted 2740 reviews so far...\n",
            "Extracted 2750 reviews so far...\n",
            "Extracted 2760 reviews so far...\n",
            "Extracted 2770 reviews so far...\n",
            "Extracted 2780 reviews so far...\n",
            "Extracted 2790 reviews so far...\n",
            "Extracted 2800 reviews so far...\n",
            "Extracted 2810 reviews so far...\n",
            "Extracted 2820 reviews so far...\n",
            "Extracted 2830 reviews so far...\n",
            "Extracted 2840 reviews so far...\n",
            "Extracted 2850 reviews so far...\n",
            "Extracted 2860 reviews so far...\n",
            "Extracted 2870 reviews so far...\n",
            "Extracted 2880 reviews so far...\n",
            "Extracted 2890 reviews so far...\n",
            "Extracted 2900 reviews so far...\n",
            "Extracted 2910 reviews so far...\n",
            "Extracted 2920 reviews so far...\n",
            "Extracted 2930 reviews so far...\n",
            "Extracted 2940 reviews so far...\n",
            "Extracted 2950 reviews so far...\n",
            "Extracted 2960 reviews so far...\n",
            "Extracted 2970 reviews so far...\n",
            "Extracted 2980 reviews so far...\n",
            "Extracted 2990 reviews so far...\n",
            "Extracted 3000 reviews so far...\n",
            "Extracted 3010 reviews so far...\n",
            "Extracted 3020 reviews so far...\n",
            "Extracted 3030 reviews so far...\n",
            "Extracted 3040 reviews so far...\n",
            "Extracted 3050 reviews so far...\n",
            "Extracted 3060 reviews so far...\n",
            "Extracted 3070 reviews so far...\n",
            "Extracted 3080 reviews so far...\n",
            "Extracted 3090 reviews so far...\n",
            "Extracted 3100 reviews so far...\n",
            "Extracted 3110 reviews so far...\n",
            "Extracted 3120 reviews so far...\n",
            "Extracted 3130 reviews so far...\n",
            "Extracted 3140 reviews so far...\n",
            "Extracted 3150 reviews so far...\n",
            "Extracted 3160 reviews so far...\n",
            "Extracted 3170 reviews so far...\n",
            "Extracted 3180 reviews so far...\n",
            "Extracted 3190 reviews so far...\n",
            "Extracted 3200 reviews so far...\n",
            "Extracted 3210 reviews so far...\n",
            "Extracted 3220 reviews so far...\n",
            "Extracted 3230 reviews so far...\n",
            "Extracted 3240 reviews so far...\n",
            "Extracted 3250 reviews so far...\n",
            "Extracted 3260 reviews so far...\n",
            "Extracted 3270 reviews so far...\n",
            "Extracted 3280 reviews so far...\n",
            "Extracted 3290 reviews so far...\n",
            "Extracted 3300 reviews so far...\n",
            "Extracted 3310 reviews so far...\n",
            "Extracted 3320 reviews so far...\n",
            "Extracted 3330 reviews so far...\n",
            "Extracted 3340 reviews so far...\n",
            "Extracted 3350 reviews so far...\n",
            "Extracted 3360 reviews so far...\n",
            "Extracted 3370 reviews so far...\n",
            "Extracted 3380 reviews so far...\n",
            "Extracted 3390 reviews so far...\n",
            "Extracted 3400 reviews so far...\n",
            "Extracted 3410 reviews so far...\n",
            "Extracted 3420 reviews so far...\n",
            "Extracted 3430 reviews so far...\n",
            "Extracted 3440 reviews so far...\n",
            "Extracted 3450 reviews so far...\n",
            "Extracted 3460 reviews so far...\n",
            "Extracted 3470 reviews so far...\n",
            "Extracted 3480 reviews so far...\n",
            "Extracted 3490 reviews so far...\n",
            "Extracted 3500 reviews so far...\n",
            "Extracted 3510 reviews so far...\n",
            "Extracted 3520 reviews so far...\n",
            "Extracted 3530 reviews so far...\n",
            "Extracted 3540 reviews so far...\n",
            "Extracted 3550 reviews so far...\n",
            "Extracted 3560 reviews so far...\n",
            "Extracted 3570 reviews so far...\n",
            "Extracted 3580 reviews so far...\n",
            "Extracted 3590 reviews so far...\n",
            "Extracted 3600 reviews so far...\n",
            "Extracted 3610 reviews so far...\n",
            "Extracted 3620 reviews so far...\n",
            "Extracted 3630 reviews so far...\n",
            "Extracted 3640 reviews so far...\n",
            "Extracted 3650 reviews so far...\n",
            "Extracted 3660 reviews so far...\n",
            "Extracted 3670 reviews so far...\n",
            "Extracted 3680 reviews so far...\n",
            "Extracted 3690 reviews so far...\n",
            "Extracted 3700 reviews so far...\n",
            "Extracted 3710 reviews so far...\n",
            "Extracted 3720 reviews so far...\n",
            "Extracted 3730 reviews so far...\n",
            "Extracted 3740 reviews so far...\n",
            "Extracted 3750 reviews so far...\n",
            "Extracted 3760 reviews so far...\n",
            "Extracted 3770 reviews so far...\n",
            "Extracted 3780 reviews so far...\n",
            "Extracted 3790 reviews so far...\n",
            "Extracted 3800 reviews so far...\n",
            "Extracted 3810 reviews so far...\n",
            "Extracted 3820 reviews so far...\n",
            "Extracted 3830 reviews so far...\n",
            "Extracted 3830 reviews so far...\n",
            "Extracted 3850 reviews so far...\n",
            "Extracted 3860 reviews so far...\n",
            "Extracted 3870 reviews so far...\n",
            "Extracted 3880 reviews so far...\n",
            "Extracted 3890 reviews so far...\n",
            "Extracted 3900 reviews so far...\n",
            "Extracted 3910 reviews so far...\n",
            "Extracted 3920 reviews so far...\n",
            "Extracted 3930 reviews so far...\n",
            "Extracted 3940 reviews so far...\n",
            "Extracted 3950 reviews so far...\n",
            "Extracted 3960 reviews so far...\n",
            "Extracted 3970 reviews so far...\n",
            "Extracted 3980 reviews so far...\n",
            "Extracted 3990 reviews so far...\n",
            "Extracted 4000 reviews so far...\n",
            "Extracted 4010 reviews so far...\n",
            "Extracted 4020 reviews so far...\n",
            "Extracted 4030 reviews so far...\n",
            "Extracted 4040 reviews so far...\n",
            "Extracted 4040 reviews so far...\n",
            "Extracted 4060 reviews so far...\n",
            "Extracted 4070 reviews so far...\n",
            "Extracted 4080 reviews so far...\n",
            "Extracted 4090 reviews so far...\n",
            "Extracted 4100 reviews so far...\n",
            "Extracted 4110 reviews so far...\n",
            "Extracted 4120 reviews so far...\n",
            "Extracted 4130 reviews so far...\n",
            "Extracted 4140 reviews so far...\n",
            "Extracted 4150 reviews so far...\n",
            "Extracted 4160 reviews so far...\n",
            "Extracted 4170 reviews so far...\n",
            "Extracted 4180 reviews so far...\n",
            "Extracted 4190 reviews so far...\n",
            "Extracted 4200 reviews so far...\n",
            "Extracted 4210 reviews so far...\n",
            "Extracted 4220 reviews so far...\n",
            "Extracted 4230 reviews so far...\n",
            "Extracted 4240 reviews so far...\n",
            "Extracted 4250 reviews so far...\n",
            "Extracted 4260 reviews so far...\n",
            "Extracted 4270 reviews so far...\n",
            "Extracted 4280 reviews so far...\n",
            "Extracted 4290 reviews so far...\n",
            "Extracted 4300 reviews so far...\n",
            "Extracted 4310 reviews so far...\n",
            "Extracted 4320 reviews so far...\n",
            "Extracted 4330 reviews so far...\n",
            "Extracted 4340 reviews so far...\n",
            "Extracted 4350 reviews so far...\n",
            "Extracted 4360 reviews so far...\n",
            "Extracted 4370 reviews so far...\n",
            "Extracted 4380 reviews so far...\n",
            "Extracted 4390 reviews so far...\n",
            "Extracted 4400 reviews so far...\n",
            "Extracted 4410 reviews so far...\n",
            "Extracted 4420 reviews so far...\n",
            "Extracted 4430 reviews so far...\n",
            "Extracted 4440 reviews so far...\n",
            "Extracted 4450 reviews so far...\n",
            "Extracted 4460 reviews so far...\n",
            "Extracted 4464 reviews so far...\n",
            "No more 'Load More' button or all reviews loaded.\n",
            "Saved all reviews to costco_reviews_with_timestamps.csv.\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# URL of the page to scrape\n",
        "url = \"https://www.influenster.com/reviews/costco/reviews\"\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "options = webdriver.ChromeOptions()\n",
        "# Temporarily disable headless mode for debugging (you can enable it after debugging)\n",
        "# options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(url)\n",
        "\n",
        "# List to store all reviews\n",
        "all_reviews = []\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        # Wait for reviews to load\n",
        "        WebDriverWait(driver, 30).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, \"Review_review__body-text__7LQFd\"))\n",
        "        )\n",
        "\n",
        "        # Parse the page content\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        reviews = soup.find_all(\"div\", class_=\"Review_review__body-text__7LQFd\")\n",
        "        timestamps = soup.find_all(\"div\", class_=\"MiniProfileTimestamp_mini-profile-timestamp__profile-age__74YIa\")\n",
        "\n",
        "        # Extract reviews and timestamps\n",
        "        for review, timestamp in zip(reviews, timestamps):\n",
        "            review_text = review.get_text(strip=True)\n",
        "            review_time = timestamp.find(\"time\")[\"datetime\"]  # Extract the datetime attribute\n",
        "\n",
        "            # Avoid duplicates\n",
        "            if not any(r['review_text'] == review_text and r['timestamp'] == review_time for r in all_reviews):\n",
        "                all_reviews.append({\"review_text\": review_text, \"timestamp\": review_time})\n",
        "\n",
        "        print(f\"Extracted {len(all_reviews)} reviews so far...\")\n",
        "\n",
        "        # Try to find and click the \"Load More\" button\n",
        "        try:\n",
        "            load_more_button = WebDriverWait(driver, 10).until(\n",
        "                EC.element_to_be_clickable((By.CLASS_NAME, \"InfiniteScroll_infinite-scroll__load-more-button____P4C\"))\n",
        "            )\n",
        "            # Scroll to the button to make it visible\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more_button)\n",
        "            load_more_button.click()\n",
        "            time.sleep(3)  # Wait for new content to load\n",
        "        except Exception as e:\n",
        "            print(\"No more 'Load More' button or all reviews loaded.\")\n",
        "            break\n",
        "\n",
        "finally:\n",
        "    driver.quit()\n",
        "\n",
        "# Save all reviews to a CSV file\n",
        "df = pd.DataFrame(all_reviews)\n",
        "df.to_csv(\"costco_reviews_with_timestamps.csv\", index=False)\n",
        "print(\"Saved all reviews to costco_reviews_with_timestamps.csv.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QctEdfygCU7c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwfXQsnXHHOD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTsZrcGwPPzZ"
      },
      "outputs": [],
      "source": [
        "pip install pydantic_ai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wiPhVq2RSDZ"
      },
      "outputs": [],
      "source": [
        "pip install transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7W8nyp_TSPf"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wed5cbRNVH7X"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace with your Hugging Face token\n",
        "huggingface_token = \"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\"\n",
        "login(token=huggingface_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoprvvMZTaFm"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "\n",
        "# Step 1: Load the Llama 2 Model\n",
        "def load_llama2_model():\n",
        "    print(\"Loading Llama 2 model...\")\n",
        "    model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Update model size as needed\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "    print(\"Model loaded successfully.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Step 2: Fetch HTML Content\n",
        "def fetch_html_text(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches the HTML text from a given URL.\n",
        "    Args:\n",
        "        url (str): The page's URL to fetch the HTML text from.\n",
        "    Returns:\n",
        "        str: The HTML text from the given URL.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "    }\n",
        "    response = requests.get(url, headers=headers, timeout=20)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch HTML from {url}. Status code: {response.status_code}\")\n",
        "        return \"\"\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# Step 3: Parse HTML Content with Llama 2\n",
        "def parse_html_with_llama2(model, tokenizer, html_content: str):\n",
        "    \"\"\"\n",
        "    Parses HTML content using Llama 2 to extract relevant information.\n",
        "\n",
        "    Args:\n",
        "        model: The Llama 2 model.\n",
        "        tokenizer: The tokenizer for Llama 2.\n",
        "        html_content (str): The HTML content of the page.\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted data as a list of dictionaries.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an intelligent assistant that extracts product information from HTML content.\n",
        "Given the raw HTML, extract the following fields:\n",
        "- Product Name\n",
        "- Brand Name\n",
        "- Price (if available)\n",
        "- Rating Count (if available)\n",
        "\n",
        "HTML content:\n",
        "{html_content}\n",
        "\n",
        "Return the output as a list of dictionaries with these keys: 'product_name', 'brand_name', 'price', 'rating_count'.\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.7)\n",
        "\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"Response from Llama 2:\", response_text)\n",
        "\n",
        "    try:\n",
        "        return eval(response_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing Llama 2 response: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Step 4: Save Parsed Data to CSV\n",
        "def save_to_csv(parsed_data):\n",
        "    \"\"\"\n",
        "    Save parsed data to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        parsed_data (list): Extracted data to save.\n",
        "    \"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    file_name = f\"parsed_data_{timestamp}.csv\"\n",
        "    df = pd.DataFrame(parsed_data)\n",
        "    df.to_csv(file_name, index=False)\n",
        "    print(f\"Data saved to {file_name}\")\n",
        "\n",
        "\n",
        "# Step 5: Main Function\n",
        "def main():\n",
        "    url = \"https://chi.vervewine.com/collections/all\"  # Update to your target URL\n",
        "    print(f\"Fetching HTML content from: {url}\")\n",
        "    html_content = fetch_html_text(url)\n",
        "\n",
        "    if not html_content:\n",
        "        print(\"Failed to fetch HTML content.\")\n",
        "        return\n",
        "\n",
        "    # Load the Llama 2 model and tokenizer\n",
        "    model, tokenizer = load_llama2_model()\n",
        "\n",
        "    print(\"Parsing HTML content with Llama 2...\")\n",
        "    parsed_data = parse_html_with_llama2(model, tokenizer, html_content)\n",
        "\n",
        "    if not parsed_data:\n",
        "        print(\"No data was extracted. Please verify the prompt or the HTML structure.\")\n",
        "        return\n",
        "\n",
        "    # Save the extracted data\n",
        "    save_to_csv(parsed_data)\n",
        "\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6IPj0gkTiyj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIKfB0_DhkeW"
      },
      "outputs": [],
      "source": [
        "# Upgrade required libraries\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjaAnbLXhINI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Step 1: Load the Llama 2 Model in 4-bit Precision\n",
        "def load_llama2_model():\n",
        "    print(\"Loading Llama 2 model in 4-bit precision...\")\n",
        "    model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "    # Configuration for 4-bit precision\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(\"Model loaded successfully.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Step 2: Fetch HTML Content\n",
        "def fetch_html_text(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches the HTML text from a given URL.\n",
        "    Args:\n",
        "        url (str): The page's URL to fetch the HTML text from.\n",
        "    Returns:\n",
        "        str: The HTML text from the given URL.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "    }\n",
        "    response = requests.get(url, headers=headers, timeout=20)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch HTML from {url}. Status code: {response.status_code}\")\n",
        "        return \"\"\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# Step 3: Parse HTML Content with Llama 2\n",
        "def parse_html_with_llama2(model, tokenizer, html_content: str):\n",
        "    \"\"\"\n",
        "    Parses HTML content using Llama 2 to extract relevant information.\n",
        "\n",
        "    Args:\n",
        "        model: The Llama 2 model.\n",
        "        tokenizer: The tokenizer for Llama 2.\n",
        "        html_content (str): The HTML content of the page.\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted data as a list of dictionaries.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "You are an intelligent assistant that extracts product information from HTML content.\n",
        "Given the raw HTML, extract the following fields:\n",
        "- Product Name\n",
        "- Brand Name\n",
        "- Price (if available)\n",
        "- Rating Count (if available)\n",
        "\n",
        "HTML content:\n",
        "{html_content}\n",
        "\n",
        "Return the output as a list of dictionaries with these keys: 'product_name', 'brand_name', 'price', 'rating_count'.\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.7)\n",
        "\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"Response from Llama 2:\", response_text)\n",
        "\n",
        "    try:\n",
        "        return eval(response_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing Llama 2 response: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "# Step 4: Save Parsed Data to CSV\n",
        "def save_to_csv(parsed_data):\n",
        "    \"\"\"\n",
        "    Save parsed data to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        parsed_data (list): Extracted data to save.\n",
        "    \"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    file_name = f\"parsed_data_{timestamp}.csv\"\n",
        "    df = pd.DataFrame(parsed_data)\n",
        "    df.to_csv(file_name, index=False)\n",
        "    print(f\"Data saved to {file_name}\")\n",
        "\n",
        "\n",
        "# Step 5: Main Function\n",
        "def main():\n",
        "    url = \"https://chi.vervewine.com/collections/all\"  # Update to your target URL\n",
        "    print(f\"Fetching HTML content from: {url}\")\n",
        "    html_content = fetch_html_text(url)\n",
        "\n",
        "    if not html_content:\n",
        "        print(\"Failed to fetch HTML content.\")\n",
        "        return\n",
        "\n",
        "    # Load the Llama 2 model and tokenizer\n",
        "    model, tokenizer = load_llama2_model()\n",
        "\n",
        "    print(\"Parsing HTML content with Llama 2...\")\n",
        "    parsed_data = parse_html_with_llama2(model, tokenizer, html_content)\n",
        "\n",
        "    if not parsed_data:\n",
        "        print(\"No data was extracted. Please verify the prompt or the HTML structure.\")\n",
        "        return\n",
        "\n",
        "    # Save the extracted data\n",
        "    save_to_csv(parsed_data)\n",
        "\n",
        "\n",
        "# Run the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Huqy9Ir-hVBt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}