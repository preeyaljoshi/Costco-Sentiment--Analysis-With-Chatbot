{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17315a3-d501-4ba7-841f-d3184107d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.7.4)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f249616-3faa-4cb5-b969-accd7f1f222d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "491d450f-75ea-46ea-abb3-c3ea0d055ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch: 2025-02-03 11:03:13.135461\n",
      "Fetched 6720 new records. Total records: 74600\n",
      "Waiting for the next run...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m fetch_reddit_data()  \n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for the next run...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id='FczWyq90wYvFm0GeyXVw7g',\n",
    "    client_secret='dHvhxFImaLAQW72Xd25V09kWBAg_BQ',\n",
    "    user_agent='CostcoScraper'\n",
    ")\n",
    "\n",
    "SUBREDDIT_NAME = 'Costco'\n",
    "FILE_NAME = '/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/reddit_costco_posts_and_comments_1000.csv'\n",
    "\n",
    "def fetch_reddit_data():\n",
    "    print(f\"Starting data fetch: {datetime.now()}\")\n",
    "    subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
    "    posts_data = []\n",
    "\n",
    "    if os.path.exists(FILE_NAME):\n",
    "        existing_df = pd.read_csv(FILE_NAME)\n",
    "        existing_ids = set(existing_df['post_id'])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        existing_ids = set()\n",
    "\n",
    "    for submission in subreddit.hot(limit=1000): \n",
    "        if submission.id not in existing_ids: \n",
    "            post_id = submission.id\n",
    "            post_title = submission.title\n",
    "            post_url = submission.url\n",
    "            post_score = submission.score\n",
    "            post_created_utc = submission.created_utc\n",
    "\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list():\n",
    "                comment_data = {\n",
    "                    'post_id': post_id,\n",
    "                    'post_title': post_title,\n",
    "                    'post_url': post_url,\n",
    "                    'post_score': post_score,\n",
    "                    'post_created': post_created_utc,\n",
    "                    'comment_id': comment.id,\n",
    "                    'comment_body': comment.body,\n",
    "                    'comment_author': comment.author.name if comment.author else 'deleted',\n",
    "                    'comment_score': comment.score,\n",
    "                    'comment_created': comment.created_utc\n",
    "                }\n",
    "                posts_data.append(comment_data)\n",
    "\n",
    "    new_df = pd.DataFrame(posts_data)\n",
    "    if not new_df.empty:\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        updated_df.to_csv(FILE_NAME, index=False)\n",
    "        print(f\"Fetched {len(new_df)} new records. Total records: {len(updated_df)}\")\n",
    "    else:\n",
    "        print(\"No new data found.\")\n",
    "\n",
    "start_time = time.time()\n",
    "max_duration = 6 * 60 * 60  \n",
    "\n",
    "while time.time() - start_time < max_duration:\n",
    "    fetch_reddit_data()  \n",
    "    print(\"Waiting for the next run...\")\n",
    "    time.sleep(60 * 60)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce5880-0984-4637-b029-926eede79fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3998fa-7a42-4511-898f-c248c4ca31f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch: 2025-02-03 11:05:16.698788\n",
      "Fetched 140 new records. Total records: 12171\n",
      "Waiting for the next run...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m fetch_reddit_data()  \u001b[38;5;66;03m# Fetch new Reddit data\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for the next run...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id='FczWyq90wYvFm0GeyXVw7g',\n",
    "    client_secret='dHvhxFImaLAQW72Xd25V09kWBAg_BQ',\n",
    "    user_agent='CostcoScraper'\n",
    ")\n",
    "\n",
    "# Define subreddit and file name\n",
    "SUBREDDIT_NAME = 'CostcoWholesale'\n",
    "FILE_NAME = '/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/reddit_costco_wholesale_posts_and_comments_1000.csv'\n",
    "\n",
    "# Function to fetch Reddit data\n",
    "def fetch_reddit_data():\n",
    "    print(f\"Starting data fetch: {datetime.now()}\")\n",
    "    subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
    "    posts_data = []\n",
    "\n",
    "    # Load existing data\n",
    "    if os.path.exists(FILE_NAME):\n",
    "        existing_df = pd.read_csv(FILE_NAME)\n",
    "        existing_ids = set(existing_df['post_id'])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        existing_ids = set()\n",
    "\n",
    "    for submission in subreddit.hot(limit=1000):  # Fetch top 1000 posts\n",
    "        if submission.id not in existing_ids:  # Skip posts that are already in the dataset\n",
    "            post_id = submission.id\n",
    "            post_title = submission.title\n",
    "            post_url = submission.url\n",
    "            post_score = submission.score\n",
    "            post_created_utc = submission.created_utc\n",
    "\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list():\n",
    "                comment_data = {\n",
    "                    'post_id': post_id,\n",
    "                    'post_title': post_title,\n",
    "                    'post_url': post_url,\n",
    "                    'post_score': post_score,\n",
    "                    'post_created': post_created_utc,\n",
    "                    'comment_id': comment.id,\n",
    "                    'comment_body': comment.body,\n",
    "                    'comment_author': comment.author.name if comment.author else 'deleted',\n",
    "                    'comment_score': comment.score,\n",
    "                    'comment_created': comment.created_utc\n",
    "                }\n",
    "                posts_data.append(comment_data)\n",
    "\n",
    "    # Create DataFrame and append new records to the existing dataset\n",
    "    new_df = pd.DataFrame(posts_data)\n",
    "    if not new_df.empty:\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        updated_df.to_csv(FILE_NAME, index=False)\n",
    "        print(f\"Fetched {len(new_df)} new records. Total records: {len(updated_df)}\")\n",
    "    else:\n",
    "        print(\"No new data found.\")\n",
    "\n",
    "# Periodic execution for a limited time (e.g., 6 hours)\n",
    "start_time = time.time()\n",
    "max_duration = 3 * 60 * 60  # 6 hours in seconds\n",
    "\n",
    "while time.time() - start_time < max_duration:\n",
    "    fetch_reddit_data()  # Fetch new Reddit data\n",
    "    print(\"Waiting for the next run...\")\n",
    "    time.sleep(60 * 60)  # Wait for 1 hour before the next fetch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db8f28-2153-41f0-85a1-fe2fcbaa339e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f349f59d-36c1-424f-b629-a4fd8418080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data fetch: 2025-02-03 11:06:26.475091\n",
      "Fetched 197 new records. Total records: 10079\n",
      "Waiting for the next run...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m fetch_reddit_data()  \u001b[38;5;66;03m# Fetch new Reddit data\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for the next run...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id='FczWyq90wYvFm0GeyXVw7g',\n",
    "    client_secret='dHvhxFImaLAQW72Xd25V09kWBAg_BQ',\n",
    "    user_agent='CostcoScraper'\n",
    ")\n",
    "\n",
    "# Define subreddit and file name\n",
    "SUBREDDIT_NAME = 'Costco_alcohol'\n",
    "FILE_NAME = '/Users/lakshitgupta/Library/CloudStorage/OneDrive-SeattleUniversity/Quater5/Capstone/Datasets/reddit_costco_alcohol_posts_and_comments.csv'\n",
    "\n",
    "# Function to fetch Reddit data\n",
    "def fetch_reddit_data():\n",
    "    print(f\"Starting data fetch: {datetime.now()}\")\n",
    "    subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
    "    posts_data = []\n",
    "\n",
    "    # Load existing data\n",
    "    if os.path.exists(FILE_NAME):\n",
    "        existing_df = pd.read_csv(FILE_NAME)\n",
    "        existing_ids = set(existing_df['post_id'])\n",
    "    else:\n",
    "        existing_df = pd.DataFrame()\n",
    "        existing_ids = set()\n",
    "\n",
    "    for submission in subreddit.hot(limit=1000):  # Fetch top 1000 posts\n",
    "        if submission.id not in existing_ids:  # Skip posts that are already in the dataset\n",
    "            post_id = submission.id\n",
    "            post_title = submission.title\n",
    "            post_url = submission.url\n",
    "            post_score = submission.score\n",
    "            post_created_utc = submission.created_utc\n",
    "\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            for comment in submission.comments.list():\n",
    "                comment_data = {\n",
    "                    'post_id': post_id,\n",
    "                    'post_title': post_title,\n",
    "                    'post_url': post_url,\n",
    "                    'post_score': post_score,\n",
    "                    'post_created': post_created_utc,\n",
    "                    'comment_id': comment.id,\n",
    "                    'comment_body': comment.body,\n",
    "                    'comment_author': comment.author.name if comment.author else 'deleted',\n",
    "                    'comment_score': comment.score,\n",
    "                    'comment_created': comment.created_utc\n",
    "                }\n",
    "                posts_data.append(comment_data)\n",
    "\n",
    "    # Create DataFrame and append new records to the existing dataset\n",
    "    new_df = pd.DataFrame(posts_data)\n",
    "    if not new_df.empty:\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        updated_df.to_csv(FILE_NAME, index=False)\n",
    "        print(f\"Fetched {len(new_df)} new records. Total records: {len(updated_df)}\")\n",
    "    else:\n",
    "        print(\"No new data found.\")\n",
    "\n",
    "# Periodic execution for a limited time (e.g., 6 hours)\n",
    "start_time = time.time()\n",
    "max_duration = 2 * 60 * 60  # 6 hours in seconds\n",
    "\n",
    "while time.time() - start_time < max_duration:\n",
    "    fetch_reddit_data()  # Fetch new Reddit data\n",
    "    print(\"Waiting for the next run...\")\n",
    "    time.sleep(60 * 60)  # Wait for 1 hour before the next fetch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db13b8-4a1e-4da3-9c7f-621ae9335b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
